import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE 
from qiskit.circuit import QuantumCircuit, Parameter
from qiskit.quantum_info import Statevector
from joblib import dump, Parallel, delayed

# Step 1: Load the Data
data = pd.read_csv('D:\\second_module\\precise_insurance_data2.csv')

# Data Preview
print("Data Preview:")
print(data.head())

# Step 2: Data Preprocessing
features = data.drop(['risk_level'], axis=1)  # Features
target = data['risk_level'].map({'High': 2, 'Medium': 1, 'Low': 0})  # Map classes to integers

numeric_features = [col for col in features.columns if features[col].dtype in ['int64', 'float64']]
categorical_features = [col for col in features.columns if features[col].dtype == 'object']

# Step 3: Split Dataset
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# Step 4: Preprocessing Pipelines
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features)
])

# Step 5: Preprocess the Data
X_train_preprocessed = preprocessor.fit_transform(X_train)
X_test_preprocessed = preprocessor.transform(X_test)

# Step 6: Define the Quantum Kernel Function
def variational_quantum_kernel(x1, x2):
    n_qubits = len(x1)
    circuit = QuantumCircuit(n_qubits)
    params = [Parameter(f'Î¸{i}') for i in range(n_qubits)]

    # Apply multiple rotation layers and entanglement
    for _ in range(2):
        for i in range(n_qubits):
            circuit.ry(params[i], i)
        for i in range(n_qubits - 1):
            circuit.cx(i, i + 1)

    # Assign parameters and evolve statevectors
    circuit_x1 = circuit.assign_parameters({params[i]: x1[i] for i in range(n_qubits)}, inplace=False)
    circuit_x2 = circuit.assign_parameters({params[i]: x2[i] for i in range(n_qubits)}, inplace=False)

    state1 = Statevector.from_int(0, 2**n_qubits).evolve(circuit_x1)
    state2 = Statevector.from_int(0, 2**n_qubits).evolve(circuit_x2)

    # Return squared magnitude of dot product
    return np.abs(state1.data.conj().dot(state2.data))**2

# Step 7: Compute Quantum Kernel Matrix (With Respect to Training Data)
def compute_variational_kernel(X1, X2):
    n_samples1, n_samples2 = len(X1), len(X2)
    kernel_matrix = np.zeros((n_samples1, n_samples2))

    results = Parallel(n_jobs=-1)(
        delayed(variational_quantum_kernel)(X1[i], X2[j])
        for i in range(n_samples1)
        for j in range(n_samples2)
    )

    index = 0
    for i in range(n_samples1):
        for j in range(n_samples2):
            kernel_matrix[i, j] = results[index]
            index += 1

    return kernel_matrix

# Compute Kernel Matrices (Training and Testing)
print("Computing the quantum kernel matrix for training data...")
X_train_vqkernel = compute_variational_kernel(X_train_preprocessed, X_train_preprocessed)

print("Computing the quantum kernel matrix for test data...")
X_test_vqkernel = compute_variational_kernel(X_test_preprocessed, X_train_preprocessed)

# Step 8: Use SMOTE for Class Imbalance
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vqkernel, y_train)

# Verify shapes after SMOTE
print(f'Shape after SMOTE - Features: {X_train_resampled.shape}, Target: {y_train_resampled.shape}')

# Step 9: Hyperparameter Tuning with Grid Search
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, None],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3)
grid_search.fit(X_train_resampled, y_train_resampled)

classifier = grid_search.best_estimator_
print(f'Best Parameters: {grid_search.best_params_}')

# Step 10: Make Predictions and Evaluate
y_pred = classifier.predict(X_test_vqkernel)

# Step 11: Model Evaluation
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy with Variational Quantum Kernel: {accuracy:.2f}')

print("Classification Report:\n", 
      classification_report(y_test, y_pred, target_names=['Low', 'Medium', 'High'], zero_division=1))

# Step 12: Save the Trained Model
dump(classifier, 'D:\\second_module\\quantum_kernel_random_forest_classifier.pkl')
print("Model saved as 'quantum_kernel_random_forest_classifier.pkl'")













from flask import Flask, render_template, request, redirect, url_for
import numpy as np
import pandas as pd
import joblib
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.decomposition import TruncatedSVD

# Load the trained model
model = joblib.load('D:\\second_module\\quantum_kernel_random_forest_classifier.pkl')

# Define numeric and categorical features
numeric_features = [
    'age', 'annual_income', 'vehicle_age', 'engine_size', 
    'mileage_driven_annually', 'accident_history', 'traffic_violations', 
    'claims_history', 'license_duration'
]
categorical_features = ['gender', 'residential_location', 'vehicle_make']

# Preprocessing pipelines
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])

# Fit transformers and save the preprocessor
def fit_and_save_preprocessor(training_data_path):
    training_data = pd.read_csv(training_data_path)
    preprocessed_training_data = preprocessor.fit_transform(training_data)
    
    # Save the fitted preprocessor
    joblib.dump(preprocessor, 'D:\\second_module\\fitted_preprocessor.pkl')

# Initialize Flask app
app = Flask(__name__)

# Call this function to fit and save the preprocessor once
fit_and_save_preprocessor('precise_insurance_data2.csv')

@app.route('/', methods=['GET', 'POST'])
def collect_data():
    if request.method == 'POST':
        data = {
            'age': float(request.form.get('age', 0)),
            'annual_income': float(request.form.get('annual_income', 0)),
            'vehicle_age': float(request.form.get('vehicle_age', 0)),
            'engine_size': float(request.form.get('engine_size', 0)),
            'mileage_driven_annually': float(request.form.get('mileage_driven_annually', 0)),
            'accident_history': float(request.form.get('accident_history', 0)),
            'traffic_violations': float(request.form.get('traffic_violations', 0)),
            'claims_history': float(request.form.get('claims_history', 0)),
            'license_duration': float(request.form.get('license_duration', 0)),
            'gender': request.form.get('gender', ''),
            'residential_location': request.form.get('residential_location', ''),
            'vehicle_make': request.form.get('vehicle_make', '')
        }

        df = pd.DataFrame([data])

        preprocessor = joblib.load('D:\\second_module\\fitted_preprocessor.pkl')
        data_preprocessed = preprocessor.transform(df)

        svd = joblib.load('D:\\second_module\\svd_transformer.pkl')
        kernel_vector = svd.transform(data_preprocessed)

        # Predict the risk level
        prediction = model.predict(kernel_vector)
        
        # Log prediction for debugging
        print(f"Input data: {data}")
        print(f"Prediction: {prediction}")

        # Assuming prediction outputs the risk level directly
        risk_level = prediction[0]  # Adjust if needed

        return redirect(url_for('show_result', risk_level=risk_level))

    return render_template('data_collection.html')


@app.route('/result')
def show_result():
    risk_level = request.args.get('risk_level', None)
    return render_template('result.html', risk_level=risk_level)

if __name__ == '__main__':
    app.run(debug=True)
